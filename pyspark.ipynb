{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVA7n1x2S6y"
      },
      "source": [
        "# **Day 1 (chapters 1 & 2 from Spark:The definitive guide)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyDG7la62VPY"
      },
      "source": [
        "1. What is bigdata?\n",
        "\n",
        "   Big Data is a collection of data that is huge in volume, yet growing        exponentially with time. It is a data with so large size and complexity that   none of traditional data management tools can store it or process it efficiently. Big data is also a data but with huge size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNxeHZbB3vUC"
      },
      "source": [
        "2. Why spark?\n",
        "\n",
        "  Apache Spark is a popular open-source data processing framework used for big  data processing and analysis. It is chosen because of its:\n",
        "\n",
        "\n",
        "\n",
        "*   Fast processing speed.\n",
        "*   In-memory processing capabilities.\n",
        "\n",
        "*   Easy to use APIs in multiple programming languages.\n",
        "*   Support for batch, real-time stream and interactive processing.\n",
        "\n",
        "*   Built-in libraries for machine learning, graph processing and SQL.\n",
        "*   Ability to scale and run on a cluster of computers, making it highly scalable and fault-tolerant.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoczeDX44--N"
      },
      "source": [
        "3. What is spark?\n",
        "\n",
        " Apache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools. These two qualities are key to the worlds of big data and machine learning, which require the marshalling of massive computing power to crunch through large data stores. Spark also takes some of the programming burdens of these tasks off the shoulders of developers with an easy-to-use API that abstracts away much of the grunt work of distributed computing and big data processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiP_Wm5W5fnu"
      },
      "source": [
        "4. Internals of spark?\n",
        "   \n",
        "   Spark's internal architecture is based on the following components:\n",
        "*   Resilient Distributed Datasets (RDDs): The basic unit of data in Spark, they are distributed collections of data that can be processed in parallel.\n",
        "\n",
        "*   Cluster Manager: Responsible for allocating resources and scheduling tasks on a cluster of machines. Spark supports multiple cluster managers such as standalone, Apache Mesos, Hadoop YARN, and Kubernetes.\n",
        "\n",
        "*   Driver Program: Coordinates and launches tasks on a cluster. It runs the main function and creates RDDs and transformations.\n",
        "\n",
        "*   Executors: The processes that run on worker nodes and execute the tasks assigned to them by the driver program.\n",
        "\n",
        "*   Task: The unit of work that is assigned to a single executor, it consists of processing data from an RDD partition.\n",
        "\n",
        "*   Shuffle: The process of redistributing data between partitions to support certain operations like join and grouping.\n",
        "\n",
        "*   DAG Scheduler: Determines the stages of tasks and the dependencies between them to create an optimized execution plan.\n",
        "\n",
        "*   Task Scheduler: Schedules tasks on executors based on the DAG and resource availability.\n",
        "\n",
        "*   Block Manager: Manages the storage and retrieval of RDD partitions and other data in memory and disk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwY9kB9VUUg3"
      },
      "source": [
        "5. Highlevel API of spark? Sparksession, Dataframe, Partitions, Transformation, Actions, Lazy Evaluation\n",
        "\n",
        "   **SparkSession** \n",
        "   \n",
        "   It was introduced in version 2.0, It is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession.\n",
        "\n",
        "  **DataFrame** \n",
        "\n",
        "  It is a Dataset organized into named columns.\n",
        "*   It is conceptually equivalent to a table in a relational database or a data\n",
        "frame in R/Python.\n",
        "\n",
        "*   To use self-defined functions, you need to “register” them with Spark.\n",
        "\n",
        "*   DataFrame is untyped, i.e., typing is checked at runtime.\n",
        "*   DataFrame is more performance-optimal than Dataset.\n",
        "\n",
        " **Partitions**\n",
        "\n",
        " To process DataFrames/Datasets in parallel, Spark breaks up the data\n",
        "into chunks called partitions, each of which is processed within an\n",
        "executor.\n",
        "\n",
        "\n",
        "\n",
        "*   Users usually do not manipulate partitions manually or individually, but\n",
        "simply specify high-level transformations of data in the physical partitions.\n",
        "*   Spark determines how this work will actually execute on the cluster.\n",
        "\n",
        "\n",
        "*   Users can specify the number of partitions and re-partition the collection if\n",
        "necessary.\n",
        "\n",
        " **Transformations and Actions**\n",
        "\n",
        " Spark DataFrame/DataSet support two types of operations:\n",
        "\n",
        " transformations and actions.\n",
        "\n",
        " **Transformations** are operations on DataFrames/DataSets that return a\n",
        "new DataFrame/DataSet\n",
        "E.g., select(), groupBy(), map() and filter().\n",
        "\n",
        " **Actions** are operations that return a result to the driver program or\n",
        "write it to storage, and kick off a computation\n",
        " E.g., show(), count() and first().\n",
        "\n",
        " **Return type difference:** transformations return DataFrames/DataSets,\n",
        "whereas actions return some other data type.\n",
        "Spark treats the two operations very differently\n",
        "\n",
        " **Lazy Evaluation**\n",
        "\n",
        " Transformations are lazily evaluated, meaning that Spark will not begin\n",
        "to execute them until it sees an action.\n",
        "Instead, Spark internally records metadata to indicate that some\n",
        "transformation operations have been requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_T7F2PzQpd9",
        "outputId": "4eb780b6-46c5-440c-b2f7-3b557542f6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=2f1734fe8226d55a1c5eb04661cf4d97a3d432af9343f163f54477314a9abbf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fc-qPYcQuWE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Assignment\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0llqrEsNeu5"
      },
      "source": [
        "#**Day 2(chapter 4,5)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73BwpMh5cRDK"
      },
      "source": [
        "**1. Structured API - Dataframes, SQL and Dataset**\n",
        "   \n",
        "   Spark SQL is a library for structured data processing which provides SQL like API on top of spark stack it supports relational data processing and SQL literal syntax to perform operations on data.\n",
        "\n",
        " Like an RDD, a DataFrame and DataSet is an immutable distributed collection of data. Unlike an RDD, data in DataSet is organized into named columns, like a table in a relational database.\n",
        "\n",
        " DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction, it also provides a domain specific language API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cMYXk-YN1xC"
      },
      "source": [
        "#2. Basic Structured Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IcfKvhRaoz"
      },
      "source": [
        "#2.1 Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOsuM2BDREPi",
        "outputId": "66ab335e-5efb-49f7-8536-799866155861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.read.format(\"json\")\\\n",
        ".load(\"2015-summary.json\")\\\n",
        ".schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcTtWKCRUKt",
        "outputId": "e841f94b-cfe4-4025-e095-98cd6934e878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"count\", LongType(), False)\n",
        "])\n",
        "df = spark.read.format(\"json\")\\\n",
        ".schema(myManualSchema)\\\n",
        ".load(\"2015-summary.json\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7I_L2gjSKHi"
      },
      "source": [
        "#2.2 Columns and Expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qp7SUHcR_42",
        "outputId": "ee8a20c3-eeb1-4363-81cc-82285fdacd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "|   15|\n",
            "|    1|\n",
            "|  344|\n",
            "|   15|\n",
            "|   62|\n",
            "|    1|\n",
            "|   62|\n",
            "|  588|\n",
            "|   40|\n",
            "|    1|\n",
            "|  325|\n",
            "|   39|\n",
            "|   64|\n",
            "|    1|\n",
            "|   41|\n",
            "|   30|\n",
            "|    6|\n",
            "|    4|\n",
            "|  230|\n",
            "|    1|\n",
            "+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, column\n",
        "df.select(col(\"count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOUc19aGTg5l",
        "outputId": "4b6d9b37-cbee-4804-85e9-8e6cd93d3468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column<'((((count + 5) * 200) - 6) < count)'>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "expr(\"(((count + 5) * 200) - 6) < count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLbkpZ4gTzXy",
        "outputId": "b7445972-9fc2-4de0-f21f-52be12e0b911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.read.format(\"json\")\\\n",
        ".load(\"2015-summary.json\")\\\n",
        ".columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8zzljqAUCNO"
      },
      "source": [
        "\n",
        "#2.3 Records and Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFrzcg_fT-wo",
        "outputId": "edd309f2-620b-4ea7-81dc-83d541e79144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkcdQKUSUOnQ"
      },
      "source": [
        "#2.4 Create Rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x3tGAbBUMUZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "myRow = Row(\"Hello\", None, 1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IFOHF_ynUZOj",
        "outputId": "1bf0a793-9fbe-490c-f01e-5e92a8920fd0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "myRow[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml2XUVtCUdIv",
        "outputId": "338cbb4f-e970-4a06-b9df-f6ab411250ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "myRow[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlP5LnkzUsUf"
      },
      "source": [
        "#2.6 Creating Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ybzJp2zUeYG"
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"JSON\")\\\n",
        ".load(\"2015-summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46q27RHzUyny",
        "outputId": "f2dbcfb7-e51c-410d-fc08-cf5bf5f4a3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----+----+\n",
            "| col1|col2|col3|\n",
            "+-----+----+----+\n",
            "|Hello|null|   1|\n",
            "+-----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructField, StructType,\\\n",
        " StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"col1\", StringType(), True),\n",
        "StructField(\"col2\", StringType(), True),\n",
        "StructField(\"col3\", LongType(), False)\n",
        "])\n",
        "myRow = Row(\"Hello\", None, 1)\n",
        "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
        "myDf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkOEcd4EVJC5"
      },
      "source": [
        "#2.7 select and selectExpr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfbKDm87VEQQ",
        "outputId": "021fa302-f5b9-471f-f26a-f5c2db0902c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|    United States|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ouhkTblVUVa",
        "outputId": "76e5eb49-1ce8-4c1b-afd1-5167f93a0a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "+-----------------+-------------------+\n",
            "|    United States|            Romania|\n",
            "|    United States|            Croatia|\n",
            "+-----------------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\n",
        "\"DEST_COUNTRY_NAME\",\n",
        "\"ORIGIN_COUNTRY_NAME\")\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmuGtUBVaDk",
        "outputId": "89bf5e21-df43-429a-8cbb-5a836dd4b6b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "+-----------------+-----------------+-----------------+\n",
            "|    United States|    United States|    United States|\n",
            "|    United States|    United States|    United States|\n",
            "+-----------------+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "df.select(\n",
        "expr(\"DEST_COUNTRY_NAME\"),\n",
        "col(\"DEST_COUNTRY_NAME\"),\n",
        "column(\"DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl5HEgQRVlrp",
        "outputId": "65de60d2-b6bc-463f-deab-69e769b5ab4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "+-----------------+-----------------+\n",
            "|    United States|    United States|\n",
            "|    United States|    United States|\n",
            "+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FayMmKjkV11v",
        "outputId": "958a3994-d083-4861-8b01-7731bbe08056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|  destination|\n",
            "+-------------+\n",
            "|United States|\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xv2uygxWmNR",
        "outputId": "68885f80-f318-450c-93f0-ddeca7e37527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|    United States|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\n",
        "expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kihMypPPWqm0",
        "outputId": "37bb8156-a67d-43c4-8646-def1e70b7d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----------------+\n",
            "|newColumnName|DEST_COUNTRY_NAME|\n",
            "+-------------+-----------------+\n",
            "|United States|    United States|\n",
            "|United States|    United States|\n",
            "+-------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\n",
        "\"DEST_COUNTRY_NAME as newColumnName\",\n",
        "\"DEST_COUNTRY_NAME\"\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiu1Et_fXBe8",
        "outputId": "3dde6ccf-57c9-4f87-f642-4c2a7f54db9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\n",
        "\"*\", \n",
        "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" )\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2HAO2wzXSBA",
        "outputId": "cd0cfdaa-26ad-4a91-fd01-9b3dde52e8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------------------------------+\n",
            "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
            "+-----------+---------------------------------+\n",
            "|1770.765625|                              132|\n",
            "+-----------+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxmQDVYLXbaK",
        "outputId": "74d6c3da-c6f5-41db-f055-80146468bfbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
            "+-----------------+-------------------+-----+---+\n",
            "|    United States|            Romania|   15|  1|\n",
            "|    United States|            Croatia|    1|  1|\n",
            "+-----------------+-------------------+-----+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.select(\n",
        "expr(\"*\"),\n",
        "lit(1).alias(\"One\")\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M516JzWwXqr0"
      },
      "source": [
        "#2.8 Adding columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpitOpsFXliN",
        "outputId": "a3379f39-af77-4418-f689-b6e18d24db9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "|    United States|            Romania|   15|        1|\n",
            "|    United States|            Croatia|    1|        1|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"numberOne\", lit(1)).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRzsrWrBX3Dk",
        "outputId": "cbeb1ab5-e9c2-49bc-ae8d-8381ce7fdf49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|    United States|            Romania|   15|        false|\n",
            "|    United States|            Croatia|    1|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\n",
        "\"withinCountry\",\n",
        "expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hsWklnYAWa"
      },
      "source": [
        "#2.9 Renaming columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctTVD-yLX-Ej",
        "outputId": "1e07a85a-d32c-4ace-fd45-4b96303061a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAkj2T1cYKIO",
        "outputId": "80e4fb0f-d4ee-4919-9234-ba116107a13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+---------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
            "+-----------------+-------------------+-----+---------------------+\n",
            "|    United States|            Romania|   15|              Romania|\n",
            "|    United States|            Croatia|    1|              Croatia|\n",
            "+-----------------+-------------------+-----+---------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfWithLongColName = df\\\n",
        ".withColumn(\n",
        "\"This Long Column-Name\",\n",
        "expr(\"ORIGIN_COUNTRY_NAME\"))\n",
        "dfWithLongColName.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaAgCES5YbEs",
        "outputId": "f7e27350-46f8-4862-ae59-a9c6880613fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+-------+\n",
            "|This Long Column-Name|new col|\n",
            "+---------------------+-------+\n",
            "|              Romania|Romania|\n",
            "|              Croatia|Croatia|\n",
            "+---------------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfWithLongColName\\\n",
        ".selectExpr(\n",
        "\"`This Long Column-Name`\",\n",
        "\"`This Long Column-Name` as `new col`\" )\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqzDfDoNYsGx",
        "outputId": "889cf00b-156f-41ca-825a-729997afac13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This Long Column-Name']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AT3AkAcYvVX",
        "outputId": "7abebc3f-8b74-43ed-c185-c69e98430003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'count']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX0UkeJQY27s",
        "outputId": "948257a7-7032-4b55-a89c-19024e164221"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[count: bigint, This Long Column-Name: string]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgyxndtQZFpg"
      },
      "source": [
        "#2.10 Changing column's type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DRIS4ccY-ij",
        "outputId": "3d87e8c5-9a16-4565-8b57-80cd044f26d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT8NhdGnZSLF",
        "outputId": "553bd4bb-8dc9-4161-ad27-660f4c14895c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"count\",col(\"count\").cast(\"int\")).printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDvSg8-ZfTi"
      },
      "source": [
        "#2.11 Filtering rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LojlYxlMZbYh"
      },
      "outputs": [],
      "source": [
        "colCondition = df.filter(col(\"count\") < 2).take(2)\n",
        "conditional = df.where(\"count < 2\").take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWs62882Zq5O",
        "outputId": "ee9fcea8-f647-433b-96e2-7ff12be3129e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n",
            "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n"
          ]
        }
      ],
      "source": [
        "print(colCondition)\n",
        "print(conditional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iooesUrOZuEU",
        "outputId": "8d1d2aa0-6e2d-4c00-bcb0-5d71d614aa03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|          Singapore|    1|\n",
            "|          Moldova|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(col(\"count\") < 2)\\\n",
        ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5A-YiwaDnX"
      },
      "source": [
        "#2.12 Getting unique rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuD9BMUPaAek",
        "outputId": "18cb7a18-25ec-4630-f35e-90ffbd918377"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtR9s1zKaNi4",
        "outputId": "400c213d-81f9-4447-df52-e922ddda442d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDO5g-YjaYcK"
      },
      "source": [
        "#2.13 Sorting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yT-PXIraRbv",
        "outputId": "6bf456a1-c8f0-43c4-9596-a5e2d565dde9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|               Malta|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "|       United States|          Singapore|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "|           Cyprus|      United States|    1|\n",
            "|         Djibouti|      United States|    1|\n",
            "|        Indonesia|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "|           Cyprus|      United States|    1|\n",
            "|         Djibouti|      United States|    1|\n",
            "|        Indonesia|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(\"count\").show(5)\n",
        "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
        "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6zCSqyxakYE",
        "outputId": "eb3fc6df-d2aa-45da-b18e-e7349487629f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|          Moldova|      United States|    1|\n",
            "|    United States|            Croatia|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+-----------------+-------------------+------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
            "+-----------------+-------------------+------+\n",
            "|    United States|      United States|370002|\n",
            "|    United States|             Canada|  8483|\n",
            "+-----------------+-------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\")).show(2)\n",
        "df.orderBy(desc(col(\"count\")), asc(col(\"DEST_COUNTRY_NAME\"))).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIh-yU4ua98j"
      },
      "source": [
        "#2.14 Limit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7rHCcvTatiN",
        "outputId": "15d1d957-0555-4b27-e218-07c2be74acef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4OEe20jaz__",
        "outputId": "508c28c0-6615-4b8d-c65e-038790d3d9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|               Malta|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "|       United States|          Singapore|    1|\n",
            "|             Moldova|      United States|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.orderBy(expr(\"count desc\")).limit(6).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RH0SR9da_2t"
      },
      "source": [
        "#2.15 Repartition and Coalesce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOhQdVXa3oN",
        "outputId": "8c804644-df01-489d-ddf7-b2a53210c138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHfjfYNbI-y",
        "outputId": "d29924a9-b547-4f78-c67e-5b48d76a9326"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.repartition(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBeKeQHNbSDK",
        "outputId": "cd8d09b5-f69d-42dd-87cb-8e618688f467"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT8WH34IbWVV",
        "outputId": "4e6ea695-7e85-40b6-8f62-760ea3a897c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Yle-AbbZ4l",
        "outputId": "bc08a875-14f1-4cb8-a2a1-33c9afc90363"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwpSjfTNPNl2"
      },
      "source": [
        "#Day 3 (chapter 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ2gcpwGb_wa"
      },
      "source": [
        "#1. Aggregation Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_RMmei5cLeg"
      },
      "source": [
        " # 1.1 count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9x2-EWWbeYP",
        "outputId": "eaa9ac6e-51ec-4dd9-e106-068685441488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkes69nucmFe"
      },
      "source": [
        "#1.2 countDistinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c429_u55cfep",
        "outputId": "af84e360-0ebc-426f-ba85-b50b06336ff6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "125"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3C4XG9_csGp"
      },
      "source": [
        "#1.3 first and last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMHxatRAcjub",
        "outputId": "0e949d00-b0ab-4761-8cda-0b9fecc435d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuWS_JK8c0Dc",
        "outputId": "5ba6d4b4-c6a6-4a55-bc76-2b7abb54a350"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30)]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK57-nTfdQRL"
      },
      "source": [
        "#1.4 min and max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHXPKqL_dnMo",
        "outputId": "ac47a860-46f2-49ae-c01a-d55e85d96600"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(min(count)=1)]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.functions import min\n",
        "df.select(min(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQssr1KRc1c3",
        "outputId": "3974fdf5-fa2b-4ba4-c844-bd7101087bc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.functions import max\n",
        "df.select(max(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpKKLd-OdtiI"
      },
      "source": [
        "#1.5 sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFWSknxkdkXC",
        "outputId": "74a8556f-8c7f-42bb-c4fa-1aab72412747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|    453316|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "df.select(sum(\"count\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_bdIbrbfNOn"
      },
      "source": [
        "#1.6 sumDistinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul1i3Q6ifMw-",
        "outputId": "ad5ead88-6d4a-470b-d54c-1869ffd73a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|sum(DISTINCT count)|\n",
            "+-------------------+\n",
            "|             450718|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum_distinct\n",
        "\n",
        "df.select(sum_distinct(\"count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDg-f_geydT",
        "outputId": "9313b3c4-dd99-4b77-ccac-4f7f0ec1bd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "| avg(count)|\n",
            "+-----------+\n",
            "|1770.765625|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df.select(avg(\"count\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHF3w-64fqFl"
      },
      "source": [
        "#1.7 grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjrs8kPOfio-"
      },
      "outputs": [],
      "source": [
        "df1= df.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
        "df2 = df.groupBy(\"ORIGIN_COUNTRY_NAME\").count()\n",
        "df3 = df.groupBy(\"count\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh_yfPV8f9ck",
        "outputId": "40ce1a50-bfa6-4c12-b60c-6d8e9fd1fdef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----+\n",
            "|DEST_COUNTRY_NAME|count|\n",
            "+-----------------+-----+\n",
            "|         Anguilla|    1|\n",
            "|           Russia|    1|\n",
            "|         Paraguay|    1|\n",
            "+-----------------+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------------------+-----+\n",
            "|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-----+\n",
            "|           Paraguay|    1|\n",
            "|             Russia|    1|\n",
            "|           Anguilla|    1|\n",
            "+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----+-----+\n",
            "|count|count|\n",
            "+-----+-----+\n",
            "|   26|    2|\n",
            "|  442|    1|\n",
            "|   19|    3|\n",
            "+-----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.show(3)\n",
        "df2.show(3)\n",
        "df3.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVFr3xFDgXoB"
      },
      "source": [
        "# 1.8 Window Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIYDHB3ZgBma",
        "outputId": "eeddaecd-b64b-486b-bc15-8f9d90bed84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        " \n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_wRTtIlJBr",
        "outputId": "82f57222-c925-4203-b676-1356492c28b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|James        |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqzqSB8mlSLi",
        "outputId": "a0eaa763-b5f5-4c54-8c55-2285a41d9d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|   1|\n",
            "|        Scott|   Finance|  3300|   2|\n",
            "|          Jen|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|         Jeff| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|       Robert|     Sales|  4100|   3|\n",
            "|         Saif|     Sales|  4100|   3|\n",
            "|      Michael|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCLJ8yyWlTBR",
        "outputId": "c9e50279-5dcd-4ab7-f636-615ac416a1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|       Robert|     Sales|  4100|         2|\n",
            "|         Saif|     Sales|  4100|         2|\n",
            "|      Michael|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFDTX8oPlXnT",
        "outputId": "1db023cf-3ef5-4d3b-e4c3-773d0d323e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|        Maria|   Finance|  3000|         0.0|\n",
            "|        Scott|   Finance|  3300|         0.5|\n",
            "|          Jen|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|         Jeff| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|       Robert|     Sales|  4100|         0.5|\n",
            "|         Saif|     Sales|  4100|         0.5|\n",
            "|      Michael|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZbKgWotla1m",
        "outputId": "0814ff22-8ec8-4027-a4e4-9d468cb5f5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|        Maria|   Finance|  3000|    1|\n",
            "|        Scott|   Finance|  3300|    1|\n",
            "|          Jen|   Finance|  3900|    2|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|         Jeff| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|       Robert|     Sales|  4100|    1|\n",
            "|         Saif|     Sales|  4100|    2|\n",
            "|      Michael|     Sales|  4600|    2|\n",
            "+-------------+----------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import ntile\n",
        "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-koDVOildWc",
        "outputId": "a8993d2c-de50-48fb-cc26-92dbbb976f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|         cume_dist|\n",
            "+-------------+----------+------+------------------+\n",
            "|        Maria|   Finance|  3000|0.3333333333333333|\n",
            "|        Scott|   Finance|  3300|0.6666666666666666|\n",
            "|          Jen|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|         Jeff| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|       Robert|     Sales|  4100|               0.8|\n",
            "|         Saif|     Sales|  4100|               0.8|\n",
            "|      Michael|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import cume_dist    \n",
        "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
        "   .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHrfefSmlgq_",
        "outputId": "db78799f-3f4b-4067-fa9e-0018beeaabcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary| lag|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|null|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|3000|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|       Robert|     Sales|  4100|3000|\n",
            "|         Saif|     Sales|  4100|3000|\n",
            "|      Michael|     Sales|  4600|4100|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lag    \n",
        "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
        "      .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH6QeiyZlj07",
        "outputId": "47150497-74de-479d-e20d-5de707b06da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|3900|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|null|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|       Robert|     Sales|  4100|4600|\n",
            "|         Saif|     Sales|  4100|null|\n",
            "|      Michael|     Sales|  4600|null|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lead    \n",
        "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Od5Zd9ilmXm",
        "outputId": "39d98930-fccb-4a46-cf41-43268088bfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
        "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3wLA9baU1jP"
      },
      "outputs": [],
      "source": [
        "summ1=spark.read.csv(\"/content/2010-summary.csv\",header=True,inferSchema=True)\n",
        "summ2=spark.read.csv(\"/content/2011-summary.csv\",header=True,inferSchema=True)\n",
        "summ3=spark.read.csv(\"/content/2012-summary.csv\",header=True,inferSchema=True)\n",
        "summ4=spark.read.csv(\"/content/2013-summary.csv\",header=True,inferSchema=True)\n",
        "summ5=spark.read.csv(\"/content/2014-summary.csv\",header=True,inferSchema=True)\n",
        "summ6=spark.read.csv(\"/content/2015-summary.csv\",header=True,inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0J3MDqpVB5o",
        "outputId": "10eb5581-9a22-434e-be0f-bb30719a7b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
            "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n"
          ]
        }
      ],
      "source": [
        "print(summ1.columns)\n",
        "print(summ2.columns)\n",
        "print(summ3.columns)\n",
        "print(summ4.columns)\n",
        "print(summ5.columns)\n",
        "print(summ6.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7OuNwI5iIP-"
      },
      "source": [
        "# **Day 4 (chapter 8)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc_NRnx8XL8Q"
      },
      "source": [
        "# 1. Joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5212IJj4XZbS"
      },
      "source": [
        "# 1.1 Try out examples for each types of joins\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_vHvC6WO15"
      },
      "source": [
        "# Inner Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuLhJ6JVEWF",
        "outputId": "1569fb49-8c0b-4629-fda5-c095622b23d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|    United States|            Romania|    1|               Haiti|  197|\n",
            "|    United States|            Romania|    1|       French Guiana|   11|\n",
            "|    United States|            Romania|    1|Saint Kitts and N...|  120|\n",
            "|    United States|            Romania|    1| Trinidad and Tobago|  213|\n",
            "|    United States|            Romania|    1|             Bolivia|   51|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inner_join = summ1.join(summ2, [\"DEST_COUNTRY_NAME\"], \"inner\")\n",
        "inner_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8OBtahAWbkE"
      },
      "source": [
        "# Left Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYZWTVrmWaZx",
        "outputId": "6c51ab90-d9ce-4d31-cbf4-a21b2670fd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|    United States|            Croatia|    1|               Haiti|  186|\n",
            "|    United States|            Croatia|    1|       French Guiana|    3|\n",
            "|    United States|            Croatia|    1|Saint Kitts and N...|  115|\n",
            "|    United States|            Croatia|    1|             Bolivia|   13|\n",
            "|    United States|            Croatia|    1| Trinidad and Tobago|  184|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_outer_join = summ3.join(summ4, [\"DEST_COUNTRY_NAME\"], \"left_outer\")\n",
        "left_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO7PndDOWoIC"
      },
      "source": [
        "## Right Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dm5ch6yWqzo",
        "outputId": "5134708f-ed9d-4e83-8b68-6669af8de66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "|    United States|               Haiti|  193|            Romania|   15|\n",
            "|    United States|Saint Kitts and N...|  123|            Romania|   15|\n",
            "|    United States|       French Guiana|    4|            Romania|   15|\n",
            "|    United States|             Bolivia|   14|            Romania|   15|\n",
            "|    United States| Trinidad and Tobago|  175|            Romania|   15|\n",
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "right_outer_join = summ5.join(summ6, [\"DEST_COUNTRY_NAME\"], \"right_outer\")\n",
        "right_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtQ3tgkiW1wX"
      },
      "source": [
        "# Full Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5dpSC15W0ih",
        "outputId": "4393b399-514e-4ea8-fd8c-f698d0fe4617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "|        Afghanistan|      United States|   11|               null| null|\n",
            "|            Algeria|               null| null|      United States|    9|\n",
            "|             Angola|      United States|   14|      United States|   13|\n",
            "|           Anguilla|      United States|   21|      United States|   34|\n",
            "|Antigua and Barbuda|      United States|  123|      United States|  115|\n",
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "full_outer_join = summ1.join(summ5, [\"DEST_COUNTRY_NAME\"], \"full_outer\")\n",
        "full_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0LsJZVXgY9"
      },
      "source": [
        "# Left Semi Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAevzTZQXfsT",
        "outputId": "242369fa-a7cc-4296-a187-a98486303209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|    1|\n",
            "|    United States|            Ireland|  264|\n",
            "|    United States|              India|   69|\n",
            "|            Egypt|      United States|   24|\n",
            "|    United States|          Singapore|   25|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_semi_join = summ1.join(summ3, summ1.DEST_COUNTRY_NAME == summ3.DEST_COUNTRY_NAME, \"leftsemi\")\n",
        "left_semi_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNv70ouZFHK"
      },
      "source": [
        "# Left Anti Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW4Q51djY8R_",
        "outputId": "011977ce-7d75-4b8d-fe41-345f6cac4af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|            Malta|      United States|    1|\n",
            "|            Yemen|      United States|    1|\n",
            "|       The Gambia|      United States|    1|\n",
            "|           Guinea|      United States|    5|\n",
            "|          Croatia|      United States|    2|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_anti_join = summ2.join(summ4, summ2.DEST_COUNTRY_NAME == summ4.DEST_COUNTRY_NAME, \"leftanti\")\n",
        "left_anti_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HurtU23ZWdO"
      },
      "source": [
        "# Cross Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjA88CVwZWDk",
        "outputId": "8b43bb03-e9f6-4a5d-8476-6abb132dbf82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "|    United States|       Saint Martin|    2|    United States|       Saint Martin|    1|\n",
            "|    United States|       Saint Martin|    2|    United States|            Romania|   12|\n",
            "|    United States|       Saint Martin|    2|    United States|            Croatia|    2|\n",
            "|    United States|       Saint Martin|    2|    United States|            Ireland|  291|\n",
            "|    United States|       Saint Martin|    2|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cross_join = summ2.crossJoin(summ5)\n",
        "cross_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVCqViucZi3q"
      },
      "source": [
        "# 1.2. Handling Duplicate column names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSkwOwgZqUX"
      },
      "source": [
        "**1:** Renaming the columns: You can use the withColumnRenamed method to rename the duplicate columns before the join. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vKTTTeZgCn",
        "outputId": "23841185-67d5-4f9f-f588-caeb4e8d789e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME_1|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+\n",
            "|      United States|            Romania|    1|\n",
            "|      United States|            Ireland|  264|\n",
            "|      United States|              India|   69|\n",
            "|              Egypt|      United States|   24|\n",
            "|  Equatorial Guinea|      United States|    1|\n",
            "+-------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME_2|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+\n",
            "|      United States|       Saint Martin|    2|\n",
            "|      United States|             Guinea|    2|\n",
            "|      United States|            Croatia|    1|\n",
            "|      United States|            Romania|    3|\n",
            "|      United States|            Ireland|  268|\n",
            "+-------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ1.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_1\").show(5)\n",
        "summ2.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_2\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ8gNgY6cJUS"
      },
      "source": [
        "**2:** Using the as keyword: When selecting columns, you can use the alias method or the as keyword to give a new name to the duplicate column. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMTS5jWGcfrg",
        "outputId": "7c41544c-8cae-4bca-9888-b2609c94f645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|              Egypt|\n",
            "|  Equatorial Guinea|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ1.select(summ1[\"DEST_COUNTRY_NAME\"].alias(\"DEST_COUNTRY_NAME_1\")).show(5)\n",
        "summ2.selectExpr(\"DEST_COUNTRY_NAME as DEST_COUNTRY_NAME_1\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVZeVVVdUWJ"
      },
      "source": [
        "**3:** Using the withColumn method: You can use the withColumn method to add a new column with a new name, and then drop the original column. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrPIZ8ftdbBw",
        "outputId": "e01cfdb1-2cf7-4e73-cd11-211126b620dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----+-------------------+\n",
            "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+-----+-------------------+\n",
            "|            Romania|    1|      United States|\n",
            "|            Ireland|  264|      United States|\n",
            "|              India|   69|      United States|\n",
            "|      United States|   24|              Egypt|\n",
            "|      United States|    1|  Equatorial Guinea|\n",
            "+-------------------+-----+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ= summ1.withColumn(\"DEST_COUNTRY_NAME_1\", summ1[\"DEST_COUNTRY_NAME\"])\n",
        "summ.drop(\"DEST_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBAvL3-udx1i"
      },
      "source": [
        "**4:** Using the select method: You can use the select method to select only the columns you need from the DataFrame, which will remove the duplicate columns. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Yf7lYOdxFP",
        "outputId": "f70bc1f6-9c6f-4b03-f718-37dbc5e1b9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|count|ORIGIN_COUNTRY_NAME|\n",
            "+-------------------+-----+-------------------+\n",
            "|      United States|    1|            Romania|\n",
            "|      United States|  264|            Ireland|\n",
            "|      United States|   69|              India|\n",
            "|              Egypt|   24|      United States|\n",
            "|  Equatorial Guinea|    1|      United States|\n",
            "+-------------------+-----+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ.select(\"DEST_COUNTRY_NAME_1\",\"count\",\"ORIGIN_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCNGqgs4eSz7"
      },
      "source": [
        "**5:** Using the drop method: You can use the drop method to drop duplicate columns after join the dataframe. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clbtbvCpeR_b",
        "outputId": "9a9bac86-a233-4316-cc1b-7a08437423a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1| ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "|            Romania|    1|      United States|               Haiti|  197|\n",
            "|            Romania|    1|      United States|       French Guiana|   11|\n",
            "|            Romania|    1|      United States|Saint Kitts and N...|  120|\n",
            "|            Romania|    1|      United States| Trinidad and Tobago|  213|\n",
            "|            Romania|    1|      United States|             Bolivia|   51|\n",
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sum = summ.join(summ2, summ1.DEST_COUNTRY_NAME == summ2.DEST_COUNTRY_NAME, \"inner\")\n",
        "sum.drop(\"DEST_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUZRGGmthVL3"
      },
      "source": [
        "# 3. How spark performs joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIc5rB5hm16"
      },
      "source": [
        "In PySpark, joins are performed by the join method on a DataFrame, which takes one or more DataFrames as arguments. The basic syntax for joining two DataFrames is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_KJj4pfe_F4",
        "outputId": "f62e6654-62ed-4ab4-dc43-f0e3b334459c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "|    United States|            Romania|    1|    United States|              Uganda|    1|\n",
            "|    United States|            Romania|    1|    United States|               Haiti|  226|\n",
            "|    United States|            Romania|    1|    United States|       French Guiana|    1|\n",
            "|    United States|            Romania|    1|    United States|Saint Kitts and N...|  127|\n",
            "|    United States|            Romania|    1|    United States|            Slovakia|    1|\n",
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ1.join(summ1, summ1.DEST_COUNTRY_NAME == summ1.DEST_COUNTRY_NAME, \"inner\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOqz6GMLh5gJ"
      },
      "source": [
        "When the join method is called, Spark will perform the following steps:\n",
        "\n",
        "1. Broadcast the smaller DataFrame: If one of the DataFrames is smaller than the other, Spark will broadcast it to all the worker nodes so that it can be used for the join.\n",
        "\n",
        "2. Partition the larger DataFrame: The larger DataFrame is partitioned into smaller chunks called RDDs, which are distributed across the worker nodes.\n",
        "\n",
        "3. Shuffle the data: The data is shuffled so that all the rows with the same join key are on the same worker node. This step is necessary so that the join can be performed in parallel.\n",
        "\n",
        "4. Perform the join: Each worker node performs the join locally on its partition of the data. The join is performed based on the join condition specified in the join method.\n",
        "\n",
        "5. Collect the results: The results from all the worker nodes are collected and combined to form the final joined DataFrame.\n",
        "\n",
        "It's important to note that the performance of the join operation depends on the distribution of the data and the size of the DataFrames. If the data is not well-distributed, a large amount of data may need to be shuffled, which can cause performance issues. Additionally, if the DataFrames are very large, it may be more efficient to perform a broadcast or bucketed join, or to use a different join strategy such as map-side join."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6AGoPnBiQDN"
      },
      "source": [
        "# **Day 5 (chapter 9)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eruRH03UiW_I"
      },
      "source": [
        "# 1. Datasources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w609il1aiZO2"
      },
      "source": [
        "# 1.1. Basics of reading data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRxTJs8PixTK"
      },
      "source": [
        "The most commonly used method for reading data in PySpark is the read method of the SparkSession object.\n",
        "\n",
        "Here is an example of how to read a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_7ZbwGuhzg7"
      },
      "outputs": [],
      "source": [
        "#sum1 = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1QpxKqMi6yr"
      },
      "source": [
        "In this example, we first create a SparkSession object, and then use the read.csv method to read the CSV file located at the specified path. The header parameter is set to True so that the first row of the CSV file is used as the header, and the inferSchema parameter is set to True so that PySpark can infer the data types of the columns.\n",
        "\n",
        "You can also read data from other file formats like json, parquet etc by using spark.read.json(), spark.read.parquet() respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhfVRd6Ii_1M"
      },
      "source": [
        "# 2. Basics of write data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAsjG8DjPrI"
      },
      "source": [
        "The most commonly used method for writing data in PySpark is the write method of the DataFrame object.\n",
        "\n",
        "Here is an example of how to write a DataFrame to a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2v6yZq4jROs"
      },
      "outputs": [],
      "source": [
        "#df.write.csv(\"path/to/new_file.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntqL6U2QjVXN"
      },
      "source": [
        "In this example, we first create a SparkSession object, and then use the read.csv method to read the CSV file located at the specified path. Then we use the write.csv method to write the DataFrame to a new CSV file located at the specified path. The header parameter is set to True so that the column names will be written as the first row of the new CSV file.\n",
        "\n",
        "You can also write data to other file formats like json, parquet etc by using df.write.json(), df.write.parquet() respectively.\n",
        "Additionally you can also write data to various databases like hive, mysql etc using df.write.jdbc() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyyyrvSijYJt"
      },
      "source": [
        "# 3. CSV files - reading, writing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Km1EOASmWZQ"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#df.write.csv(\"path/to/new_file.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW3zymlbmZND"
      },
      "source": [
        "# 4. REading and writing json files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGPD8YUsmcia"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.json(\"path/to/file.json\")\n",
        "\n",
        "#df.write.json(\"path/to/new_file.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lZ5wC6emsyf"
      },
      "source": [
        "# 5. Parquet files - important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MJrVh-rnYZ2"
      },
      "source": [
        "Parquet is a columnar storage format that is widely used in the Apache Hadoop ecosystem and is supported by many big data processing frameworks, including PySpark. There are several reasons why Parquet is important in PySpark:\n",
        "\n",
        "**Efficiency:** Parquet stores data in a columnar format, which means that only the required columns are read and processed, rather than reading and processing the entire row. This leads to significant performance improvements when working with large datasets.\n",
        "\n",
        "**Compression:** Parquet supports various compression algorithms, such as Snappy and Gzip, which can greatly reduce the storage space required for large datasets.\n",
        "\n",
        "**Schema evolution:** Parquet supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset. This is particularly useful when working with data that is constantly changing or evolving.\n",
        "\n",
        "**Predicate pushdown:** Parquet supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer. This leads to further performance improvements when working with large datasets.\n",
        "\n",
        "**Interoperability:** Parquet is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82CbHienicc"
      },
      "source": [
        "# 6. Reading and Writing parquet files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwkPifqwnkHa"
      },
      "source": [
        "To read and write parquet files in PySpark you can use the read.parquet() and write.parquet() methods respectively. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwB9B39qm0F-"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.parquet(\"path/to/file.parquet\")\n",
        "\n",
        "#df.write.parquet(\"path/to/new_file.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fB78L2nsx4"
      },
      "source": [
        "# 7. orc - optional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGzSC7kwn25g"
      },
      "source": [
        "ORC (Optimized Row Columnar) is a file format that is similar to Parquet and is also widely used in the Apache Hadoop ecosystem. Like Parquet, ORC is a columnar storage format that is designed to improve the performance and storage efficiency of big data processing frameworks, such as PySpark.\n",
        "\n",
        "Here are some of the key benefits of using ORC in PySpark:\n",
        "\n",
        "**Performance:** ORC stores data in a columnar format, which leads to significant performance improvements when working with large datasets.\n",
        "\n",
        "**Compression:** ORC supports various compression algorithms, such as Snappy, Zlib, and LZO, which can greatly reduce the storage space required for large datasets.\n",
        "\n",
        "**Schema evolution:** ORC supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset.\n",
        "\n",
        "**Predicate pushdown:** ORC supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer.\n",
        "\n",
        "**Interoperability:** ORC is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhlFQDOAoTEZ"
      },
      "source": [
        "# 8. Splittable File Types and COmpression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jplwQSCGoUw4"
      },
      "source": [
        "When working with PySpark, you can specify the file type and compression algorithm when reading and writing data using the appropriate methods. For example, you can use the read.parquet() method to read a Parquet file and the write.avro() method to write an Avro file.\n",
        "\n",
        "Here is an example of reading a parquet file with snappy compression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftNmrmqtn2Ha"
      },
      "outputs": [],
      "source": [
        "#df.write.format(\"parquet\").option(\"compression\", \"snappy\").save(\"path/to/new_file.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdgUI11jo5P5"
      },
      "source": [
        "# 9. Managing File size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLgOLUjpUbW"
      },
      "source": [
        "Managing file size in PySpark is an important consideration when working with large datasets. There are several strategies that can be used to manage the file size of your data, including:\n",
        "\n",
        "**Partitioning:** Partitioning is the process of dividing a large dataset into smaller, more manageable chunks. In PySpark, you can partition your data using the partitionBy method when writing data to a file. This allows you to split your data into multiple smaller files based on a specific column, such as a date or a category.\n",
        "\n",
        "**Compression:** As mentioned previously, compression is the process of reducing the size of a dataset. This can be done by using a compression algorithm, such as Snappy, Gzip, LZO, or Zlib, when reading or writing data in PySpark.\n",
        "\n",
        "**File format:** Choosing the appropriate file format for your data can also help manage file size. Columnar file formats, such as Parquet and ORC, are often more efficient than row-based formats, such as CSV or JSON, when working with large datasets because they require less storage space and are more easily compressible.\n",
        "\n",
        "**Filtering:** Filtering is the process of removing unnecessary data from your dataset. In PySpark, you can filter your data using the filter method to remove rows that do not meet a specific criteria. This can help reduce the file size of your data by removing unneeded information.\n",
        "\n",
        "**Sampling:** Sampling is the process of selecting a random subset of your data to work with. In PySpark, you can use the sample method to randomly select a certain percentage or number of rows from your dataset. This can be useful when working with large datasets as it allows you to work with a smaller, more manageable subset of your data while still getting an accurate representation of the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDgCKyJjo8Z8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC09Brtuj3IU"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDtKBgyFgZxO",
        "outputId": "a5f876e1-c9ec-412d-d71e-01226cd08b81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lUfnGU_v1ZUw1dRrovASH9M5BgYeAyxQ\n",
            "To: /content/2015-summary.csv\n",
            "100%|██████████| 7.08k/7.08k [00:00<00:00, 3.88MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "file_id = \"1lUfnGU_v1ZUw1dRrovASH9M5BgYeAyxQ\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, \"2015-summary.csv\", quiet=False)\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2015-summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufb9owxVlew9"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcli4DDlqcg",
        "outputId": "9248bea6-0ebe-4648-dbe2-93af5e2d8564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|       United States|            Romania|   15|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|            Ireland|  344|\n",
            "|               Egypt|      United States|   15|\n",
            "|       United States|              India|   62|\n",
            "|       United States|          Singapore|    1|\n",
            "|       United States|            Grenada|   62|\n",
            "|          Costa Rica|      United States|  588|\n",
            "|             Senegal|      United States|   40|\n",
            "|             Moldova|      United States|    1|\n",
            "|       United States|       Sint Maarten|  325|\n",
            "|       United States|   Marshall Islands|   39|\n",
            "|              Guyana|      United States|   64|\n",
            "|               Malta|      United States|    1|\n",
            "|            Anguilla|      United States|   41|\n",
            "|             Bolivia|      United States|   30|\n",
            "|       United States|           Paraguay|    6|\n",
            "|             Algeria|      United States|    4|\n",
            "|Turks and Caicos ...|      United States|  230|\n",
            "|       United States|          Gibraltar|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT * FROM summary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQIrgLbhpkHK"
      },
      "source": [
        "1. What is the total number of flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U2yDFMylw0k",
        "outputId": "66762ff1-6b2f-4f35-e977-d252f9b7d90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|ROWS_count|\n",
            "+----------+\n",
            "|       256|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT COUNT(*) AS ROWS_count FROM summary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJiRJXuoYlk"
      },
      "source": [
        "2. What are the top 10 destination countries by count?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzco4lWnlyD",
        "outputId": "ca696ec6-6398-44b2-988b-ecfa24944190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----------+\n",
            "| DEST_COUNTRY_NAME|total_count|\n",
            "+------------------+-----------+\n",
            "|     United States|   411352.0|\n",
            "|            Canada|     8399.0|\n",
            "|            Mexico|     7140.0|\n",
            "|    United Kingdom|     2025.0|\n",
            "|             Japan|     1548.0|\n",
            "|           Germany|     1468.0|\n",
            "|Dominican Republic|     1353.0|\n",
            "|       South Korea|     1048.0|\n",
            "|       The Bahamas|      955.0|\n",
            "|            France|      935.0|\n",
            "+------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 10\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib8LIrvtowE4"
      },
      "source": [
        "3. How many flights originated from the United States?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tO2uHnIokC5",
        "outputId": "35f0b282-6774-4d63-f497-f3ac3beb7c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411966.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGbucQSWo7sj"
      },
      "source": [
        "4. What are the top 5 origin countries for flights to Japan?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tflM0zt3o4J7",
        "outputId": "212c4357-6a81-41c9-964f-b26f2a161b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|     1548.0|\n",
            "+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          WHERE DEST_COUNTRY_NAME = 'Japan'\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 5\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uazMY9ocpwiE"
      },
      "source": [
        "5. What is the total number of flights to the United States?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG8wFlgRpHQK",
        "outputId": "1dd0d3bb-c54f-4f2f-e7d6-dccdf6386bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411352.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE DEST_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18eyX-WSp9jw"
      },
      "source": [
        "6. What is the total number of flights from the United States?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZZWHdspp3_Q",
        "outputId": "9cbf36cd-1d94-4b7e-c504-134ad63d2be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411966.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mlRwCalqCSL"
      },
      "source": [
        "7. What are the top 10 origin and destination pairs by count?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm7two0WqBPC",
        "outputId": "930ebef1-b6f1-4663-dce7-298df7e77ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------------+-----------+\n",
            "|      United States|    United States|   370002.0|\n",
            "|             Canada|    United States|     8483.0|\n",
            "|      United States|           Canada|     8399.0|\n",
            "|             Mexico|    United States|     7187.0|\n",
            "|      United States|           Mexico|     7140.0|\n",
            "|      United States|   United Kingdom|     2025.0|\n",
            "|     United Kingdom|    United States|     1970.0|\n",
            "|      United States|            Japan|     1548.0|\n",
            "|              Japan|    United States|     1496.0|\n",
            "|      United States|          Germany|     1468.0|\n",
            "+-------------------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 10\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7dxxw9qYE3"
      },
      "source": [
        "8. How many flights originated from each country?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ozRf53qNm6",
        "outputId": "21ac518f-b254-475b-e440-fc1ec2854f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|   411966.0|\n",
            "|             Canada|     8483.0|\n",
            "|             Mexico|     7187.0|\n",
            "|     United Kingdom|     1970.0|\n",
            "|              Japan|     1496.0|\n",
            "| Dominican Republic|     1420.0|\n",
            "|            Germany|     1336.0|\n",
            "|        The Bahamas|      986.0|\n",
            "|             France|      952.0|\n",
            "|              China|      920.0|\n",
            "|           Colombia|      867.0|\n",
            "|        South Korea|      827.0|\n",
            "|            Jamaica|      712.0|\n",
            "|        Netherlands|      660.0|\n",
            "|             Brazil|      619.0|\n",
            "|         Costa Rica|      608.0|\n",
            "|        El Salvador|      508.0|\n",
            "|               Cuba|      478.0|\n",
            "|             Panama|      465.0|\n",
            "|              Spain|      442.0|\n",
            "+-------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJftYB_qsfW"
      },
      "source": [
        "9. How many flights went to each country?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ1wXYNeqhdi",
        "outputId": "ad95b390-208c-4d3c-ffbd-17507b09096b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----------+\n",
            "| DEST_COUNTRY_NAME|total_count|\n",
            "+------------------+-----------+\n",
            "|     United States|   411352.0|\n",
            "|            Canada|     8399.0|\n",
            "|            Mexico|     7140.0|\n",
            "|    United Kingdom|     2025.0|\n",
            "|             Japan|     1548.0|\n",
            "|           Germany|     1468.0|\n",
            "|Dominican Republic|     1353.0|\n",
            "|       South Korea|     1048.0|\n",
            "|       The Bahamas|      955.0|\n",
            "|            France|      935.0|\n",
            "|          Colombia|      873.0|\n",
            "|            Brazil|      853.0|\n",
            "|       Netherlands|      776.0|\n",
            "|             China|      772.0|\n",
            "|           Jamaica|      666.0|\n",
            "|        Costa Rica|      588.0|\n",
            "|       El Salvador|      561.0|\n",
            "|            Panama|      510.0|\n",
            "|              Cuba|      466.0|\n",
            "|             Spain|      420.0|\n",
            "+------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lagDxZ6hrJ7k"
      },
      "source": [
        "10. What is the total number of flights between the United States and Canada?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBvyIFD7qyCB",
        "outputId": "1c73b216-cd49-4324-d3aa-7e9a66e43b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|   16882.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'Canada') OR (ORIGIN_COUNTRY_NAME = 'Canada' AND DEST_COUNTRY_NAME = 'United States')\\\n",
        "\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I56S1Q6zrNQ7"
      },
      "source": [
        "11. What are the 5 most common origin countries for flights to the United Kingdom?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNbP55k6rCIR",
        "outputId": "4e633c65-3d75-4592-c020-6511bd9d155b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|     2025.0|\n",
            "+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "  FROM summary\\\n",
        "  WHERE DEST_COUNTRY_NAME = 'United Kingdom'\\\n",
        "  GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "  LIMIT 5\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A4b86iqrlO7"
      },
      "source": [
        "12. What are the top 10 destination countries for flights from China?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeWWZEVBrVtQ",
        "outputId": "44aadd1a-8038-4325-b8c1-05b76cba586b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------+\n",
            "|DEST_COUNTRY_NAME|total_count|\n",
            "+-----------------+-----------+\n",
            "|    United States|      920.0|\n",
            "+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\\\n",
        "  SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'China'\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "  LIMIT 10\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtD_a23lr0XT"
      },
      "source": [
        "13. What is the total number of flights between United States and New Zealand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-l0oqxgrvDt",
        "outputId": "e94929b5-83ab-4a15-85d4-39004284b128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|     185.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'New Zealand') OR (ORIGIN_COUNTRY_NAME = 'New Zealand' AND DEST_COUNTRY_NAME = 'United States')\\\n",
        "\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P6-qrMjsP4z"
      },
      "source": [
        "14. What is the total number of flights from India?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcoWUc6kr88i",
        "outputId": "ea7a2cc5-449e-4d55-ccb4-4b9db0b9cc6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|      62.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'India'\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdric_4ssimz"
      },
      "source": [
        "15. What is the rank of the destination country with the most flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BdSFeBisVwN",
        "outputId": "c46624ce-487c-4f9a-f468-9ac890a55505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----------+----+\n",
            "| DEST_COUNTRY_NAME|total_count|rank|\n",
            "+------------------+-----------+----+\n",
            "|     United States|   411352.0|   1|\n",
            "|            Canada|     8399.0|   2|\n",
            "|            Mexico|     7140.0|   3|\n",
            "|    United Kingdom|     2025.0|   4|\n",
            "|             Japan|     1548.0|   5|\n",
            "|           Germany|     1468.0|   6|\n",
            "|Dominican Republic|     1353.0|   7|\n",
            "|       South Korea|     1048.0|   8|\n",
            "|       The Bahamas|      955.0|   9|\n",
            "|            France|      935.0|  10|\n",
            "|          Colombia|      873.0|  11|\n",
            "|            Brazil|      853.0|  12|\n",
            "|       Netherlands|      776.0|  13|\n",
            "|             China|      772.0|  14|\n",
            "|           Jamaica|      666.0|  15|\n",
            "|        Costa Rica|      588.0|  16|\n",
            "|       El Salvador|      561.0|  17|\n",
            "|            Panama|      510.0|  18|\n",
            "|              Cuba|      466.0|  19|\n",
            "|             Spain|      420.0|  20|\n",
            "+------------------+-----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
        "  FROM summary\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vTeroUgs8AR"
      },
      "source": [
        "16. What is the rank of the destination country with the most flights from France?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzXxpZ6css0A",
        "outputId": "3f99e4e0-d7f1-486d-a189-81821e8e9acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------+----+\n",
            "|DEST_COUNTRY_NAME|total_count|rank|\n",
            "+-----------------+-----------+----+\n",
            "|    United States|      952.0|   1|\n",
            "+-----------------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'France'\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1-C_1H3tJ4w"
      },
      "source": [
        "17. What is the cumulative sum of flights to each destination country, ordered by the number of flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OayGJDo-tDP8",
        "outputId": "1203e3d5-4098-46d7-dde8-714568f43440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-----------+--------------+\n",
            "| DEST_COUNTRY_NAME|total_count|cumulative_sum|\n",
            "+------------------+-----------+--------------+\n",
            "|     United States|   411352.0|      411352.0|\n",
            "|            Canada|     8399.0|      419751.0|\n",
            "|            Mexico|     7140.0|      426891.0|\n",
            "|    United Kingdom|     2025.0|      428916.0|\n",
            "|             Japan|     1548.0|      430464.0|\n",
            "|           Germany|     1468.0|      431932.0|\n",
            "|Dominican Republic|     1353.0|      433285.0|\n",
            "|       South Korea|     1048.0|      434333.0|\n",
            "|       The Bahamas|      955.0|      435288.0|\n",
            "|            France|      935.0|      436223.0|\n",
            "|          Colombia|      873.0|      437096.0|\n",
            "|            Brazil|      853.0|      437949.0|\n",
            "|       Netherlands|      776.0|      438725.0|\n",
            "|             China|      772.0|      439497.0|\n",
            "|           Jamaica|      666.0|      440163.0|\n",
            "|        Costa Rica|      588.0|      440751.0|\n",
            "|       El Salvador|      561.0|      441312.0|\n",
            "|            Panama|      510.0|      441822.0|\n",
            "|              Cuba|      466.0|      442288.0|\n",
            "|             Spain|      420.0|      442708.0|\n",
            "+------------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  SUM(SUM(count)) OVER (ORDER BY SUM(count) DESC) as cumulative_sum\\\n",
        "  FROM summary\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qBeZBxrtRw5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "p0llqrEsNeu5",
        "o7I_L2gjSKHi",
        "T8zzljqAUCNO",
        "CkcdQKUSUOnQ",
        "KlP5LnkzUsUf",
        "BkOEcd4EVJC5",
        "M516JzWwXqr0",
        "x_hsWklnYAWa",
        "PgyxndtQZFpg",
        "fwDvSg8-ZfTi",
        "3e5A-YiwaDnX",
        "vDO5g-YjaYcK",
        "AIh-yU4ua98j",
        "4RH0SR9da_2t",
        "b_RMmei5cLeg",
        "dkes69nucmFe",
        "n3C4XG9_csGp",
        "rK57-nTfdQRL",
        "rpKKLd-OdtiI",
        "9_bdIbrbfNOn",
        "zHF3w-64fqFl",
        "cVFr3xFDgXoB",
        "lr_vHvC6WO15",
        "k8OBtahAWbkE",
        "qtQ3tgkiW1wX",
        "HS0LsJZVXgY9",
        "RKNv70ouZFHK",
        "7HurtU23ZWdO",
        "YVCqViucZi3q",
        "NUZRGGmthVL3",
        "w609il1aiZO2",
        "DhfVRd6Ii_1M",
        "cyyyrvSijYJt",
        "jW3zymlbmZND",
        "2lZ5wC6emsyf",
        "R82CbHienicc",
        "17fB78L2nsx4",
        "JhlFQDOAoTEZ",
        "AdgUI11jo5P5",
        "RC09Brtuj3IU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}